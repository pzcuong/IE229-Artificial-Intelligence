# -*- coding: utf-8 -*-
"""IE229 - Lab03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OHjwSk2ysxYdJdPaIrynbK9LKorF03lw

#LOAD MNIST DATASET
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
import torchvision as tv

train_dataset = tv.datasets.MNIST(root="./", train=True, transform=tv.transforms.ToTensor(), download=True)
test_dataset = tv.datasets.MNIST(root="./", train=False, transform=tv.transforms.ToTensor(), download=True)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)

for i in range(10):
  print(train_dataset[i])
  plt.imshow(train_dataset[i][0][0],cmap='gray')
  txt = "label: "+str(train_dataset[i][1])
  plt.text(2,2,txt,color="white")
  plt.show()

"""#問題 1
第 9 回の講義資料に基づいて MNIST の学習・推論プログラムを実装し .
"""

l1 = torch.nn.Linear(784,300)
l2 = torch.nn.Linear(300,10)
params = list(l1.parameters())+list(l2.parameters())
optimizer = torch.optim.Adam(params)
def mynet(x):
  h = F.relu(l1(x))
  y = l2(h)
  return y

def train():
  for e in range(10):
    loss = 0
    for images,labels in train_loader:
      images = images.view(-1,28*28)
      optimizer.zero_grad()
      y = mynet(images)
      batchloss = F.cross_entropy(y,labels)
      batchloss.backward()
      optimizer.step()
      loss = loss+batchloss.item()
    print("epoch: ", e, "loss: ", loss)

def test():
  correct = 0
  total = len(test_loader.dataset)
  for images,labels in test_loader:
    images = images.view(-1,28*28)
    y = mynet(images)
    pred_labels = y.max(dim=1)[1]
    correct = correct + (pred_labels==labels).sum()
  print("correct: ", correct.item())
  print("total: ", total)
  print("accuracy: ", correct.item()/total)

train()

test()

"""#問題 2
第 9 回の講義資料での MNIST の学習・推論プログラムは中間層が 300 次元であった。中間層
を 800 次元に変更し、そのプログラム (.py)とコンソールに表示される実行結果を word ファイルに貼り
付けて提出せよ。 (300 次元の場合に比べて誤差が大きく減る。精度は若干良くなるがだいたい同じ。 )
"""

l1 = torch.nn.Linear(784,800)
l2 = torch.nn.Linear(800,10)
params=list(l1.parameters())+list(l2.parameters())
optimizer=torch.optim.Adam(params)
def mynet(x):
  h = F.relu(l1(x))
  y = l2(h)
  return y

def train():
  for e in range(10):
    loss = 0
    for images,labels in train_loader:
      images = images.view(-1,28*28)
      optimizer.zero_grad()
      y = mynet(images)
      batchloss = F.cross_entropy(y,labels)
      batchloss.backward()
      optimizer.step()
      loss = loss+batchloss.item()
    print("epoch: ", e, "loss: ", loss)

def test():
  correct = 0
  total = len(test_loader.dataset)
  for images,labels in test_loader:
    images = images.view(-1,28*28)
    y = mynet(images)
    pred_labels = y.max(dim=1)[1]
    correct = correct + (pred_labels==labels).sum()
  print("correct: ", correct.item())
  print("total: ", total)
  print("accuracy: ", correct.item()/total)

train()

test()